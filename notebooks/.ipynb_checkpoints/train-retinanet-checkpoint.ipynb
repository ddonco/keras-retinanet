{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# show images inline\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules when they have changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os,sys,inspect\n",
    "import keras\n",
    "import keras.preprocessing.image\n",
    "import tensorflow as tf\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "# Change these to absolute imports if you copy this script outside the keras_retinanet package.\n",
    "from keras_retinanet import layers  # noqa: F401\n",
    "from keras_retinanet import losses\n",
    "from keras_retinanet import models\n",
    "from keras_retinanet.callbacks import RedirectModel\n",
    "from keras_retinanet.callbacks.eval import Evaluate\n",
    "from keras_retinanet.models.retinanet import retinanet_bbox\n",
    "from keras_retinanet.preprocessing.csv_generator import CSVGenerator\n",
    "from keras_retinanet.preprocessing.kitti import KittiGenerator\n",
    "from keras_retinanet.preprocessing.open_images import OpenImagesGenerator\n",
    "from keras_retinanet.preprocessing.pascal_voc import PascalVocGenerator\n",
    "from keras_retinanet.utils.anchors import make_shapes_callback\n",
    "from keras_retinanet.utils.config import read_config_file, parse_anchor_parameters\n",
    "from keras_retinanet.utils.keras_version import check_keras_version\n",
    "from keras_retinanet.utils.model import freeze as freeze_model\n",
    "from keras_retinanet.utils.transform import random_transform_generator\n",
    "from keras_retinanet.utils.image import random_visual_effect_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedirs(path):\n",
    "    # Intended behavior: try to create the directory,\n",
    "    # pass if the directory exists already, fails otherwise.\n",
    "    # Meant for Python 2.7/3.n compatibility.\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path):\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session():\n",
    "    \"\"\" Construct a modified tf session.\n",
    "    \"\"\"\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    return tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_with_weights(model, weights, skip_mismatch):\n",
    "    \"\"\" Load weights for model.\n",
    "    Args\n",
    "        model         : The model to load weights for.\n",
    "        weights       : The weights to load.\n",
    "        skip_mismatch : If True, skips layers whose shape of weights doesn't match with the model.\n",
    "    \"\"\"\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights, by_name=True, skip_mismatch=skip_mismatch)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(backbone_retinanet, num_classes, weights, multi_gpu=0,\n",
    "                  freeze_backbone=False, lr=1e-5, config=None):\n",
    "    \"\"\" Creates three models (model, training_model, prediction_model).\n",
    "    Args\n",
    "        backbone_retinanet : A function to call to create a retinanet model with a given backbone.\n",
    "        num_classes        : The number of classes to train.\n",
    "        weights            : The weights to load into the model.\n",
    "        multi_gpu          : The number of GPUs to use for training.\n",
    "        freeze_backbone    : If True, disables learning for the backbone.\n",
    "        config             : Config parameters, None indicates the default configuration.\n",
    "    Returns\n",
    "        model            : The base model. This is also the model that is saved in snapshots.\n",
    "        training_model   : The training model. If multi_gpu=0, this is identical to model.\n",
    "        prediction_model : The model wrapped with utility functions to perform object detection (applies regression values and performs NMS).\n",
    "    \"\"\"\n",
    "\n",
    "    modifier = freeze_model if freeze_backbone else None\n",
    "\n",
    "    # load anchor parameters, or pass None (so that defaults will be used)\n",
    "    anchor_params = None\n",
    "    num_anchors   = None\n",
    "    if config and 'anchor_parameters' in config:\n",
    "        anchor_params = parse_anchor_parameters(config)\n",
    "        num_anchors   = anchor_params.num_anchors()\n",
    "\n",
    "    # Keras recommends initialising a multi-gpu model on the CPU to ease weight sharing, and to prevent OOM errors.\n",
    "    # optionally wrap in a parallel model\n",
    "    if multi_gpu > 1:\n",
    "        from keras.utils import multi_gpu_model\n",
    "        with tf.device('/cpu:0'):\n",
    "            model = model_with_weights(backbone_retinanet(num_classes, num_anchors=num_anchors, modifier=modifier), weights=weights, skip_mismatch=True)\n",
    "        training_model = multi_gpu_model(model, gpus=multi_gpu)\n",
    "    else:\n",
    "        model          = model_with_weights(backbone_retinanet(num_classes, num_anchors=num_anchors, modifier=modifier), weights=weights, skip_mismatch=True)\n",
    "        training_model = model\n",
    "\n",
    "    # make prediction model\n",
    "    prediction_model = retinanet_bbox(model=model, anchor_params=anchor_params)\n",
    "\n",
    "    # compile model\n",
    "    training_model.compile(\n",
    "        loss={\n",
    "            'regression'    : losses.smooth_l1(),\n",
    "            'classification': losses.focal()\n",
    "        },\n",
    "        optimizer=keras.optimizers.adam(lr=lr, clipnorm=0.001)\n",
    "    )\n",
    "\n",
    "    return model, training_model, prediction_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks(model, training_model, prediction_model, validation_generator, config):\n",
    "    \"\"\" Creates the callbacks to use during training.\n",
    "    Args\n",
    "        model: The base model.\n",
    "        training_model: The model that is used for training.\n",
    "        prediction_model: The model that should be used for validation.\n",
    "        validation_generator: The generator for creating validation data.\n",
    "        config: RetinanetConfig object.\n",
    "    Returns:\n",
    "        A list of callbacks used for training.\n",
    "    \"\"\"\n",
    "    callbacks = []\n",
    "\n",
    "    tensorboard_callback = None\n",
    "\n",
    "    if config.tensorboard_dir:\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "            log_dir                = config.tensorboard_dir,\n",
    "            histogram_freq         = 0,\n",
    "            batch_size             = config.batch_size,\n",
    "            write_graph            = True,\n",
    "            write_grads            = False,\n",
    "            write_images           = False,\n",
    "            embeddings_freq        = 0,\n",
    "            embeddings_layer_names = None,\n",
    "            embeddings_metadata    = None\n",
    "        )\n",
    "        callbacks.append(tensorboard_callback)\n",
    "\n",
    "    if config.evaluation and validation_generator:\n",
    "        if config.dataset_type == 'coco':\n",
    "            from ..callbacks.coco import CocoEval\n",
    "\n",
    "            # use prediction model for evaluation\n",
    "            evaluation = CocoEval(validation_generator, tensorboard=tensorboard_callback)\n",
    "        else:\n",
    "            evaluation = Evaluate(validation_generator, tensorboard=tensorboard_callback, weighted_average=config.weighted_average)\n",
    "        evaluation = RedirectModel(evaluation, prediction_model)\n",
    "        callbacks.append(evaluation)\n",
    "\n",
    "    # save the model\n",
    "    if config.snapshots:\n",
    "        # ensure directory created first; otherwise h5py will error after epoch.\n",
    "        makedirs(config.snapshot_path)\n",
    "        checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "            os.path.join(\n",
    "                config.snapshot_path,\n",
    "                '{backbone}_{dataset_type}_{{epoch:02d}}.h5'.format(backbone=config.backbone, dataset_type=config.dataset_type)\n",
    "            ),\n",
    "            verbose=1,\n",
    "            # save_best_only=True,\n",
    "            # monitor=\"mAP\",\n",
    "            # mode='max'\n",
    "        )\n",
    "        checkpoint = RedirectModel(checkpoint, model)\n",
    "        callbacks.append(checkpoint)\n",
    "\n",
    "    callbacks.append(keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor    = 'loss',\n",
    "        factor     = 0.1,\n",
    "        patience   = 2,\n",
    "        verbose    = 1,\n",
    "        mode       = 'auto',\n",
    "        min_delta  = 0.0001,\n",
    "        cooldown   = 0,\n",
    "        min_lr     = 0\n",
    "    ))\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generators(config, preprocess_image):\n",
    "    \"\"\" Create generators for training and validation.\n",
    "    Args\n",
    "        config           : RetinanetConfig object containing configuration for generators.\n",
    "        preprocess_image : Function that preprocesses an image for the network.\n",
    "    \"\"\"\n",
    "    common_args = {\n",
    "        'batch_size'       : config.batch_size,\n",
    "        'config'           : config.config,\n",
    "        'image_min_side'   : config.image_min_side,\n",
    "        'image_max_side'   : config.image_max_side,\n",
    "        'preprocess_image' : preprocess_image,\n",
    "    }\n",
    "\n",
    "    # create random transform generator for augmenting training data\n",
    "    if config.random_transform:\n",
    "        transform_generator = random_transform_generator(\n",
    "            min_rotation=-0.1,\n",
    "            max_rotation=0.1,\n",
    "            min_translation=(-0.1, -0.1),\n",
    "            max_translation=(0.1, 0.1),\n",
    "            min_shear=-0.1,\n",
    "            max_shear=0.1,\n",
    "            min_scaling=(0.9, 0.9),\n",
    "            max_scaling=(1.1, 1.1),\n",
    "            flip_x_chance=0.5,\n",
    "            flip_y_chance=0.5,\n",
    "        )\n",
    "        visual_effect_generator = random_visual_effect_generator(\n",
    "            contrast_range=(0.9, 1.1),\n",
    "            brightness_range=(-.1, .1),\n",
    "            hue_range=(-0.05, 0.05),\n",
    "            saturation_range=(0.95, 1.05)\n",
    "        )\n",
    "    elif config.defect_transform:\n",
    "        transform_generator = random_transform_generator(\n",
    "            min_rotation=-0.1,\n",
    "            max_rotation=0.1,\n",
    "            min_translation=(-0.1, -0.1),\n",
    "            max_translation=(0.1, 0.1),\n",
    "            min_shear=-0.1,\n",
    "            max_shear=0.1,\n",
    "            flip_x_chance=0.5,\n",
    "            flip_y_chance=0.5,\n",
    "        )\n",
    "        visual_effect_generator = random_visual_effect_generator(\n",
    "            contrast_range=(0.9, 1.1),\n",
    "            brightness_range=(-.1, .1),\n",
    "            hue_range=(-0.05, 0.05),\n",
    "            saturation_range=(0.95, 1.05)\n",
    "        )\n",
    "    else:\n",
    "        transform_generator = random_transform_generator(flip_x_chance=0.5)\n",
    "        visual_effect_generator = None\n",
    "\n",
    "    if config.dataset_type == 'coco':\n",
    "        # import here to prevent unnecessary dependency on cocoapi\n",
    "        from ..preprocessing.coco import CocoGenerator\n",
    "\n",
    "        train_generator = CocoGenerator(\n",
    "            config.coco_path,\n",
    "            'train2017',\n",
    "            transform_generator=transform_generator,\n",
    "            visual_effect_generator=visual_effect_generator,\n",
    "            **common_args\n",
    "        )\n",
    "\n",
    "        validation_generator = CocoGenerator(\n",
    "            config.coco_path,\n",
    "            'val2017',\n",
    "            shuffle_groups=False,\n",
    "            **common_args\n",
    "        )\n",
    "    elif config.dataset_type == 'pascal':\n",
    "        train_generator = PascalVocGenerator(\n",
    "            config.pascal_path,\n",
    "            'trainval',\n",
    "            transform_generator=transform_generator,\n",
    "            visual_effect_generator=visual_effect_generator,\n",
    "            **common_args\n",
    "        )\n",
    "\n",
    "        validation_generator = PascalVocGenerator(\n",
    "            config.pascal_path,\n",
    "            'test',\n",
    "            shuffle_groups=False,\n",
    "            **common_args\n",
    "        )\n",
    "    elif config.dataset_type == 'csv':\n",
    "        train_generator = CSVGenerator(\n",
    "            config.csv_annotations,\n",
    "            config.csv_classes,\n",
    "            transform_generator=transform_generator,\n",
    "            visual_effect_generator=visual_effect_generator,\n",
    "            **common_args\n",
    "        )\n",
    "\n",
    "        if config.csv_val_annotations:\n",
    "            validation_generator = CSVGenerator(\n",
    "                config.csv_val_annotations,\n",
    "                config.csv_classes,\n",
    "                shuffle_groups=False,\n",
    "                **common_args\n",
    "            )\n",
    "        else:\n",
    "            validation_generator = None\n",
    "    elif config.dataset_type == 'oid':\n",
    "        train_generator = OpenImagesGenerator(\n",
    "            config.oid_main_dir,\n",
    "            subset='train',\n",
    "            version=config.oid_version,\n",
    "            labels_filter=config.oid_labels_filter,\n",
    "            annotation_cache_dir=config.oid_annotation_cache_dir,\n",
    "            parent_label=config.oid_parent_label,\n",
    "            transform_generator=transform_generator,\n",
    "            visual_effect_generator=visual_effect_generator,\n",
    "            **common_args\n",
    "        )\n",
    "\n",
    "        validation_generator = OpenImagesGenerator(\n",
    "            config.oid_main_dir,\n",
    "            subset='validation',\n",
    "            version=config.oid_version,\n",
    "            labels_filter=config.oid_labels_filter,\n",
    "            annotation_cache_dir=config.oid_annotation_cache_dir,\n",
    "            parent_label=config.oid_parent_label,\n",
    "            shuffle_groups=False,\n",
    "            **common_args\n",
    "        )\n",
    "    elif config.dataset_type == 'kitti':\n",
    "        train_generator = KittiGenerator(\n",
    "            config.kitti_path,\n",
    "            subset='train',\n",
    "            transform_generator=transform_generator,\n",
    "            visual_effect_generator=visual_effect_generator,\n",
    "            **common_args\n",
    "        )\n",
    "\n",
    "        validation_generator = KittiGenerator(\n",
    "            config.kitti_path,\n",
    "            subset='val',\n",
    "            shuffle_groups=False,\n",
    "            **common_args\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError('Invalid data type received: {}'.format(config.dataset_type))\n",
    "\n",
    "    return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_config(config):\n",
    "    \"\"\" Function to check for inherent contradictions within parsed arguments.\n",
    "    For example, batch_size < num_gpus\n",
    "    Intended to raise errors prior to backend initialisation.\n",
    "    Args\n",
    "        config: RetinanetConfig object\n",
    "    Returns\n",
    "        config\n",
    "    \"\"\"\n",
    "\n",
    "    if config.multi_gpu > 1 and config.batch_size < config.multi_gpu:\n",
    "        raise ValueError(\n",
    "            \"Batch size ({}) must be equal to or higher than the number of GPUs ({})\".format(config.batch_size,\n",
    "                                                                                             config.multi_gpu))\n",
    "\n",
    "    if config.multi_gpu > 1 and config.snapshot:\n",
    "        raise ValueError(\n",
    "            \"Multi GPU training ({}) and resuming from snapshots ({}) is not supported.\".format(config.multi_gpu,\n",
    "                                                                                                config.snapshot))\n",
    "\n",
    "    if config.multi_gpu > 1 and not config.multi_gpu_force:\n",
    "        raise ValueError(\"Multi-GPU support is experimental, use at own risk! Run with --multi-gpu-force if you wish to continue.\")\n",
    "\n",
    "    if 'resnet' not in config.backbone:\n",
    "        warnings.warn('Using experimental backbone {}. Only resnet50 has been properly tested.'.format(config.backbone))\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args):\n",
    "    \"\"\" Parse the arguments.\n",
    "    \"\"\"\n",
    "    parser     = argparse.ArgumentParser(description='Simple training script for training a RetinaNet network.')\n",
    "    subparsers = parser.add_subparsers(help='Arguments for specific dataset types.', dest='dataset_type')\n",
    "    subparsers.required = True\n",
    "\n",
    "    coco_parser = subparsers.add_parser('coco')\n",
    "    coco_parser.add_argument('coco_path', help='Path to dataset directory (ie. /tmp/COCO).')\n",
    "\n",
    "    pascal_parser = subparsers.add_parser('pascal')\n",
    "    pascal_parser.add_argument('pascal_path', help='Path to dataset directory (ie. /tmp/VOCdevkit).')\n",
    "\n",
    "    kitti_parser = subparsers.add_parser('kitti')\n",
    "    kitti_parser.add_argument('kitti_path', help='Path to dataset directory (ie. /tmp/kitti).')\n",
    "\n",
    "    def csv_list(string):\n",
    "        return string.split(',')\n",
    "\n",
    "    oid_parser = subparsers.add_parser('oid')\n",
    "    oid_parser.add_argument('main_dir', help='Path to dataset directory.')\n",
    "    oid_parser.add_argument('--version',  help='The current dataset version is v4.', default='v4')\n",
    "    oid_parser.add_argument('--labels-filter',  help='A list of labels to filter.', type=csv_list, default=None)\n",
    "    oid_parser.add_argument('--annotation-cache-dir', help='Path to store annotation cache.', default='.')\n",
    "    oid_parser.add_argument('--parent-label', help='Use the hierarchy children of this label.', default=None)\n",
    "\n",
    "    csv_parser = subparsers.add_parser('csv')\n",
    "    csv_parser.add_argument('annotations', help='Path to CSV file containing annotations for training.')\n",
    "    csv_parser.add_argument('classes', help='Path to a CSV file containing class label mapping.')\n",
    "    csv_parser.add_argument('--val-annotations', help='Path to CSV file containing annotations for validation (optional).')\n",
    "\n",
    "    group = parser.add_mutually_exclusive_group()\n",
    "    group.add_argument('--snapshot',          help='Resume training from a snapshot.')\n",
    "    group.add_argument('--imagenet-weights',  help='Initialize the model with pretrained imagenet weights. This is the default behaviour.', action='store_const', const=True, default=True)\n",
    "    group.add_argument('--weights',           help='Initialize the model with weights from a file.')\n",
    "    group.add_argument('--no-weights',        help='Don\\'t initialize the model with any weights.', dest='imagenet_weights', action='store_const', const=False)\n",
    "\n",
    "    parser.add_argument('--backbone',         help='Backbone model used by retinanet.', default='resnet50', type=str)\n",
    "    parser.add_argument('--batch-size',       help='Size of the batches.', default=1, type=int)\n",
    "    parser.add_argument('--gpu',              help='Id of the GPU to use (as reported by nvidia-smi).')\n",
    "    parser.add_argument('--multi-gpu',        help='Number of GPUs to use for parallel processing.', type=int, default=0)\n",
    "    parser.add_argument('--multi-gpu-force',  help='Extra flag needed to enable (experimental) multi-gpu support.', action='store_true')\n",
    "    parser.add_argument('--epochs',           help='Number of epochs to train.', type=int, default=50)\n",
    "    parser.add_argument('--steps',            help='Number of steps per epoch.', type=int, default=10000)\n",
    "    parser.add_argument('--lr',               help='Learning rate.', type=float, default=1e-5)\n",
    "    parser.add_argument('--snapshot-path',    help='Path to store snapshots of models during training (defaults to \\'./snapshots\\')', default='./snapshots')\n",
    "    parser.add_argument('--tensorboard-dir',  help='Log directory for Tensorboard output', default='./logs')\n",
    "    parser.add_argument('--no-snapshots',     help='Disable saving snapshots.', dest='snapshots', action='store_false')\n",
    "    parser.add_argument('--no-evaluation',    help='Disable per epoch evaluation.', dest='evaluation', action='store_false')\n",
    "    parser.add_argument('--freeze-backbone',  help='Freeze training of backbone layers.', action='store_true')\n",
    "    parser.add_argument('--random-transform', help='Randomly transform image and annotations.', action='store_true')\n",
    "    parser.add_argument('--defect-transform', help='Apply defect detection transforms to image and annotations.', action='store_true')\n",
    "    parser.add_argument('--image-min-side',   help='Rescale the image so the smallest side is min_side.', type=int, default=800)\n",
    "    parser.add_argument('--image-max-side',   help='Rescale the image if the largest side is larger than max_side.', type=int, default=1333)\n",
    "    parser.add_argument('--config',           help='Path to a configuration parameters .ini file.')\n",
    "    parser.add_argument('--weighted-average', help='Compute the mAP using the weighted average of precisions among classes.', action='store_true')\n",
    "    parser.add_argument('--compute-val-loss', help='Compute validation loss during training', dest='compute_val_loss', action='store_true')\n",
    "\n",
    "    # Fit generator arguments\n",
    "    parser.add_argument('--multiprocessing',  help='Use multiprocessing in fit_generator.', action='store_true')\n",
    "    parser.add_argument('--workers',          help='Number of generator workers.', type=int, default=1)\n",
    "    parser.add_argument('--max-queue-size',   help='Queue length for multiprocessing workers in fit_generator.', type=int, default=10)\n",
    "\n",
    "    return check_args(parser.parse_args(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    check_config(config)\n",
    "    \n",
    "    # create object that stores backbone information\n",
    "    backbone = models.backbone(config.backbone)\n",
    "\n",
    "    # make sure keras is the minimum required version\n",
    "    check_keras_version()\n",
    "\n",
    "    # optionally choose specific GPU\n",
    "    if config.gpu:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = config.gpu\n",
    "    keras.backend.tensorflow_backend.set_session(get_session())\n",
    "\n",
    "    # optionally load config parameters\n",
    "    if config.config:\n",
    "        config.config = read_config_file(config.config)\n",
    "\n",
    "    # create the generators\n",
    "    train_generator, validation_generator = create_generators(config, backbone.preprocess_image)\n",
    "\n",
    "    # create the model\n",
    "    if config.snapshot is not None:\n",
    "        print('Loading model, this may take a second...')\n",
    "        model            = models.load_model(config.snapshot, backbone_name=config.backbone)\n",
    "        training_model   = model\n",
    "        anchor_params    = None\n",
    "        if config.config and 'anchor_parameters' in config.config:\n",
    "            anchor_params = parse_anchor_parameters(config.config)\n",
    "        prediction_model = retinanet_bbox(model=model, anchor_params=anchor_params)\n",
    "    else:\n",
    "        weights = config.weights\n",
    "        # default to imagenet if nothing else is specified\n",
    "        if weights is None and config.imagenet_weights:\n",
    "            weights = backbone.download_imagenet()\n",
    "\n",
    "        print('Creating model, this may take a second...')\n",
    "        inputs = keras.layers.Input(shape=(358, 1333, 3))\n",
    "        model, training_model, prediction_model = create_models(\n",
    "            backbone_retinanet=backbone.retinanet(inputs=inputs,\n",
    "            num_classes=train_generator.num_classes(),\n",
    "            weights=weights,\n",
    "            multi_gpu=config.multi_gpu,\n",
    "            freeze_backbone=config.freeze_backbone,\n",
    "            lr=config.lr,\n",
    "            config=config.config\n",
    "        )\n",
    "\n",
    "    # print model summary\n",
    "    print(model.summary())\n",
    "\n",
    "    # this lets the generator compute backbone layer shapes using the actual backbone model\n",
    "    if 'vgg' in config.backbone or 'densenet' in config.backbone:\n",
    "        train_generator.compute_shapes = make_shapes_callback(model)\n",
    "        if validation_generator:\n",
    "            validation_generator.compute_shapes = train_generator.compute_shapes\n",
    "\n",
    "    # create the callbacks\n",
    "    callbacks = create_callbacks(\n",
    "        model,\n",
    "        training_model,\n",
    "        prediction_model,\n",
    "        validation_generator,\n",
    "        config,\n",
    "    )\n",
    "\n",
    "    if not config.compute_val_loss:\n",
    "        validation_generator = None\n",
    "\n",
    "    # start training\n",
    "    return training_model.fit_generator(\n",
    "        generator=train_generator,\n",
    "        steps_per_epoch=config.steps,\n",
    "        epochs=config.epochs,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks,\n",
    "        workers=config.workers,\n",
    "        use_multiprocessing=config.multiprocessing,\n",
    "        max_queue_size=config.max_queue_size,\n",
    "        validation_data=validation_generator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinanetConfig(object):\n",
    "    def __init__(self):\n",
    "        # Select dataset type\n",
    "        self.dataset_type = 'csv'\n",
    "        \n",
    "        # If training on COCO dataset\n",
    "        # Path to dataset directory (ie. /tmp/COCO).\n",
    "        self.coco_path = None\n",
    "\n",
    "        # If training on Pascal dataset\n",
    "        # Path to dataset directory (ie. /tmp/VOCdevkit).\n",
    "        self.pascal_path = None\n",
    "\n",
    "        # If training on Kitti dataset\n",
    "        # Path to dataset directory (ie. /tmp/kitti).\n",
    "        self.kitti_path = None\n",
    "\n",
    "        # If training on custom dataset\n",
    "        # Path to CSV file containing annotations for training.\n",
    "        self.csv_annotations = None\n",
    "        # Path to a CSV file containing class label mapping.\n",
    "        self.csv_classes = None\n",
    "        # Path to CSV file containing annotations for validation (optional).\n",
    "        self.csv_val_annotations = None\n",
    "\n",
    "        # If OID dataset\n",
    "        # Path to dataset directory.\n",
    "        self.oid_main_dir = None\n",
    "        # The current dataset version is v4.\n",
    "        self.oid_version = 'v4'\n",
    "        # A list of labels to filter.\n",
    "        self.oid_labels_filter = None\n",
    "        if self.oid_labels_filter:\n",
    "            self.oid_labels_filter = csv_list(self.oid_labels_filter)\n",
    "        # Path to store annotation cache.\n",
    "        self.oid_annotation_cache_dir = '.'\n",
    "        # Use the hierarchy children of this label.\n",
    "        self.oid_parent_label = None\n",
    "\n",
    "        # Training parameters\n",
    "        # Resume training from a snapshot.\n",
    "        self.snapshot = None\n",
    "        # Initialize the model with pretrained imagenet weights. This is the default behavior.\n",
    "        self.imagenet_weights = True\n",
    "        # Initialize the model with weights from a file.\n",
    "        self.weights = None\n",
    "        # Don't initialize the model with any weights.\n",
    "        self.no_weights = False\n",
    "        # Backbone model used by retinanet.\n",
    "        self.backbone = 'resnet50'\n",
    "        # Size of the batches.\n",
    "        self.batch_size = 1\n",
    "        # ID of the GPU to use (as reported by nvidia-smi).\n",
    "        self.gpu = None\n",
    "        # Number of GPUs to use for parallel processing.\n",
    "        self.multi_gpu = 0\n",
    "        # Extra flag needed to enable (experimental) multi-gpu support.\n",
    "        self.multi_gpu_force = False\n",
    "        # Number of epochs to train.\n",
    "        self.epochs = 50\n",
    "        # Number of steps per epoch.\n",
    "        self.steps = 10000\n",
    "        # Learning rate.\n",
    "        self.lr = 1e-5\n",
    "        # Path to store snapshots of models during training (defaults to \\'./snapshots\\').\n",
    "        self.snapshot_path = './snapshots'\n",
    "        # Log directory for Tensorboard output.\n",
    "        self.tensorboard_dir = './logs'\n",
    "        # Disable saving snapshots.\n",
    "        self.no_snapshots = False\n",
    "        # Disable per epoch evaluation.\n",
    "        self.no_evaluation = False\n",
    "        # Freeze training of backbone layers.\n",
    "        self.freeze_backbone = True\n",
    "        # Randomly transform image and annotations.\n",
    "        self.random_transform = False\n",
    "        # Apply defect detection transforms to image and annotations.\n",
    "        self.defect_transform = True\n",
    "        # Rescale the image so the smallest side is min_side.\n",
    "        self.image_min_side = 800\n",
    "        # Rescale the image if the largest side is larger than max_side.\n",
    "        self.image_max_side = 1333\n",
    "        # Path to a configuration parameters .ini file.\n",
    "        self.config = None\n",
    "        # Compute the mAP using the weighted average of precisions among classes.\n",
    "        self.weighted_average = True\n",
    "        # Compute validation loss during training.\n",
    "        self.compute_val_loss = True\n",
    "        # Use multiprocessing in fit_generator.\n",
    "        self.multiprocessing = True\n",
    "        # Number of generator workers.\n",
    "        self.workers = 1\n",
    "        # Queue length for multiprocessing workers in fit_generator.\n",
    "        self.max_queue_size = 10\n",
    "        \n",
    "    \n",
    "def csv_list(string):\n",
    "    return string.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-d5ee4cfd6885>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetinanetConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-07869a91e375>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# create the generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# create the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-6f92690cbdf6>\u001b[0m in \u001b[0;36mcreate_generators\u001b[0;34m(config, preprocess_image)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtransform_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mvisual_effect_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisual_effect_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mcommon_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         )\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/projects/keras-retinanet/keras_retinanet/preprocessing/csv_generator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_data_file, csv_class_file, base_dir, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# Take base_dir from annotations file if not explicitly specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_data_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# parse the provided class file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/posixpath.py\u001b[0m in \u001b[0;36mdirname\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;34m\"\"\"Returns the directory component of a pathname\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_sep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "config = RetinanetConfig()\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
